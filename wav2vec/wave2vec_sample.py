# -*- coding: utf-8 -*-
"""wave2vec_sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h-xLLkm_ae_Ysteei3jBjyitoe4bP1It
"""

from scipy.io import wavfile
import numpy as np

file_name = 'YAF_youth_sad.wav'  #input file

data = wavfile.read(file_name1)
framerate = data1[0]
sounddata = data1[1]
time = np.arange(0,len(sounddata))/framerate
print('Sample rate:',framerate,'Hz')
print('Total time:',len(sounddata)/framerate,'s')





import librosa
import torch
from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC



tockenizer = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")

input_audio, _ = librosa.load(file_name, sr=16000)


input_values = tokenizer(input_audio, return_tensors="pt").input_values
logits = model(input_values1).logits
predicted_ids = torch.argmax(logits, dim=-1)
transcription = tokenizer.batch_decode(predicted_ids)[0]


print(transcription)

